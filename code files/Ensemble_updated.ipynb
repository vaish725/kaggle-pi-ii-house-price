{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051bc13a",
   "metadata": {},
   "source": [
    "# Ensemble Quantile Regression Notebook\n",
    "*Generated: 2025-06-02 17:40 UTC*\n",
    "\n",
    "This notebook trains LightGBM + CatBoost quantile regressors, bags multiple seeds, applies conformal calibration, and writes a submission CSV in **assets/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f40893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# reproducibility\n",
    "SEEDS = [0, 1, 2, 3, 4]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Paths\n",
    "NB_DIR   = Path.cwd()          # assumes notebook lives in code files/\n",
    "ROOT_DIR = NB_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / 'dataset'\n",
    "ASSETS   = ROOT_DIR / 'assets'\n",
    "ASSETS.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55766684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 47) (200000, 46)\n"
     ]
    }
   ],
   "source": [
    "ID = 'id'\n",
    "TARGET = 'sale_price'\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / 'dataset.csv')\n",
    "test_df  = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7942eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkvai\\AppData\\Local\\Temp\\ipykernel_20260\\1657367271.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sale_nbr'].fillna(df['sale_nbr'].median(), inplace=True)\n",
      "C:\\Users\\vkvai\\AppData\\Local\\Temp\\ipykernel_20260\\1657367271.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sale_nbr'].fillna(df['sale_nbr'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def enrich(df):\n",
    "    # log area\n",
    "    df['log_area'] = np.log1p(df['area'])\n",
    "    # distance to Seattle CBD\n",
    "    lat0, lon0 = 47.6097, -122.3331\n",
    "    df['dist_cbd_km'] = np.sqrt((111*(df['latitude']-lat0))**2 +\n",
    "                                (85*(df['longitude']-lon0))**2)\n",
    "    # sale_warning as category\n",
    "    df['sale_warning'] = df['sale_warning'].astype(str).fillna('missing')\n",
    "    # sale_nbr numeric\n",
    "    df['sale_nbr'] = pd.to_numeric(df['sale_nbr'], errors='coerce')\n",
    "    df['sale_nbr'].fillna(df['sale_nbr'].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = enrich(train_df)\n",
    "test_df  = enrich(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa331d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(df):\n",
    "    num_cols = df.select_dtypes(['int64','float64']).columns.drop([ID, TARGET], errors='ignore')\n",
    "    cat_cols = df.select_dtypes(['object','category']).columns\n",
    "\n",
    "    # remove engineered cols from scaling\n",
    "    num_scaled = num_cols.drop(['log_area','dist_cbd_km'], errors='ignore')\n",
    "\n",
    "    numeric_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        ('num', numeric_pipe, num_scaled),\n",
    "        ('cat', cat_pipe, cat_cols),\n",
    "        ('direct', 'passthrough', ['log_area','dist_cbd_km'])\n",
    "    ])\n",
    "\n",
    "    return pre\n",
    "\n",
    "pre = build_preprocessor(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8828e4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices: (160000, 47) (200000, 47)\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "train_idx, val_idx = next(cv.split(train_df))\n",
    "X_train = train_df.iloc[train_idx].copy()\n",
    "X_val   = train_df.iloc[val_idx].copy()\n",
    "y_train = X_train.pop(TARGET)\n",
    "y_val   = X_val.pop(TARGET)\n",
    "\n",
    "X_train_t = pre.fit_transform(X_train)\n",
    "X_val_t   = pre.transform(X_val)\n",
    "X_test_t  = pre.transform(test_df)\n",
    "print('Matrices:', X_train_t.shape, X_test_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a30bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgb_quantile(X, y, a_lo=0.05, a_hi=0.95, random_state=0):\n",
    "    params = dict(\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=-1,\n",
    "        num_leaves=256,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    lo = lgb.LGBMRegressor(objective='quantile', alpha=a_lo, **params)\n",
    "    hi = lgb.LGBMRegressor(objective='quantile', alpha=a_hi, **params)\n",
    "    lo.fit(X, y)\n",
    "    hi.fit(X, y)\n",
    "    return lo, hi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7a311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 185000.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 1435000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 185000.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 1435000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 185000.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 1435000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 185000.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 1435000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 185000.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4154\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 1435000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\vkvai\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preds_lo_test, preds_hi_test = [], []\n",
    "preds_lo_val,  preds_hi_val  = [], []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    lo, hi = fit_lgb_quantile(X_train_t, y_train, 0.05, 0.95, seed)\n",
    "    preds_lo_test.append(lo.predict(X_test_t))\n",
    "    preds_hi_test.append(hi.predict(X_test_t))\n",
    "    preds_lo_val.append(lo.predict(X_val_t))\n",
    "    preds_hi_val.append(hi.predict(X_val_t))\n",
    "\n",
    "lgb_lo_test = np.mean(preds_lo_test, axis=0)\n",
    "lgb_hi_test = np.mean(preds_hi_test, axis=0)\n",
    "lgb_lo_val  = np.mean(preds_lo_val,  axis=0)\n",
    "lgb_hi_val  = np.mean(preds_hi_val,  axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5b340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lo = CatBoostRegressor(loss_function='Quantile:alpha=0.05',\n",
    "                            iterations=1300, depth=8,\n",
    "                            learning_rate=0.03,\n",
    "                            random_seed=RANDOM_STATE,\n",
    "                            verbose=False)\n",
    "cat_hi = CatBoostRegressor(loss_function='Quantile:alpha=0.95',\n",
    "                            iterations=1300, depth=8,\n",
    "                            learning_rate=0.03,\n",
    "                            random_seed=RANDOM_STATE,\n",
    "                            verbose=False)\n",
    "\n",
    "cat_lo.fit(X_train_t, y_train)\n",
    "cat_hi.fit(X_train_t, y_train)\n",
    "\n",
    "cat_lo_test = cat_lo.predict(X_test_t)\n",
    "cat_hi_test = cat_hi.predict(X_test_t)\n",
    "cat_lo_val  = cat_lo.predict(X_val_t)\n",
    "cat_hi_val  = cat_hi.predict(X_val_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b056f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_lower_raw_val = 0.5*lgb_lo_val + 0.5*cat_lo_val\n",
    "pi_upper_raw_val = 0.5*lgb_hi_val + 0.5*cat_hi_val\n",
    "\n",
    "pi_lower_raw_test = 0.5*lgb_lo_test + 0.5*cat_lo_test\n",
    "pi_upper_raw_test = 0.5*lgb_hi_test + 0.5*cat_hi_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41514ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_hat: 8437.936817926668\n",
      "Post-CQR validation coverage: 90.000%\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.10  # target 90% coverage\n",
    "scores = np.maximum(y_val - pi_upper_raw_val,\n",
    "                    pi_lower_raw_val - y_val)\n",
    "q_hat = np.quantile(scores, 1 - alpha)\n",
    "print('q_hat:', q_hat)\n",
    "\n",
    "pi_lower = (pi_lower_raw_test - q_hat).clip(min=0)\n",
    "pi_upper = np.maximum(pi_upper_raw_test + q_hat, pi_lower)\n",
    "\n",
    "coverage_val = ((y_val >= pi_lower_raw_val - q_hat) &\n",
    "                (y_val <= pi_upper_raw_val + q_hat)).mean()\n",
    "print(f'Post-CQR validation coverage: {coverage_val:.3%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9a5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def winkler(y, lo, hi, alpha=0.10):\n",
    "    \"\"\"\n",
    "    Vectorised Winkler interval score.\n",
    "    Accepts pandas Series or NumPy arrays of the same length.\n",
    "    \"\"\"\n",
    "    y  = np.asarray(y)\n",
    "    lo = np.asarray(lo)\n",
    "    hi = np.asarray(hi)\n",
    "\n",
    "    width   = hi - lo\n",
    "    over_lo = np.clip(lo - y, 0, None)   # only the part where y < lo\n",
    "    over_hi = np.clip(y - hi, 0, None)   # only the part where y > hi\n",
    "    penalty = (2 / alpha) * (over_lo + over_hi)\n",
    "\n",
    "    return width + penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742cb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Winkler (val): 337,219\n"
     ]
    }
   ],
   "source": [
    "val_winkler = winkler(y_val, \n",
    "                      pi_lower_raw_val - q_hat, \n",
    "                      pi_upper_raw_val + q_hat).mean()\n",
    "\n",
    "print(f\"Mean Winkler (val): {val_winkler:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13de8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\n",
    "    ID: test_df[ID],\n",
    "    'pi_lower': pi_lower,\n",
    "    'pi_upper': pi_upper\n",
    "})\n",
    "csv_path = ASSETS / 'ensemble_cqr_lgb_cat_v1.csv'\n",
    "sub.to_csv(csv_path, index=False)\n",
    "print('Saved submission to', csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
